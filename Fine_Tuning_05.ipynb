{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning the model\n",
        "\n",
        "The script is to fine-tune the KTP's Fall-Detection model with the newly reconstructed data.\n",
        "\n",
        "Part of the code is provided from the KTP project (Model Architecture, OFDataset)"
      ],
      "metadata": {
        "id": "qSdJYo5ZD_Y4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eeu9WckRJIbt",
        "outputId": "5ff74c0c-c5a6-4796-d895-3f66522b1173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "class FallDetectionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FallDetectionCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv3d(2, 64, (3, 3, 3), padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.conv2 = nn.Conv3d(64, 128, (3, 3, 3), padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(128)\n",
        "        self.conv3 = nn.Conv3d(128, 256, (3, 3, 3), padding=1)\n",
        "        self.bn3 = nn.BatchNorm3d(256)\n",
        "        self.conv4 = nn.Conv3d(256, 256, (3, 3, 3), padding=1)\n",
        "        self.bn4 = nn.BatchNorm3d(256)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool3d(1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.gelu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool3d(x, 2)\n",
        "        x = F.gelu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool3d(x, 2)\n",
        "        x = F.gelu(self.bn3(self.conv3(x)))\n",
        "        # x = F.max_pool3d(x, 2)\n",
        "        x = F.gelu(self.bn4(self.conv4(x)))\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.gelu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Fmd28DuiJwbM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import Compose, Normalize\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "class OpticalFlow3DDataset(Dataset):\n",
        "    def __init__(self, base_folder):\n",
        "        self.base_folder = base_folder\n",
        "        self.file_paths = []\n",
        "\n",
        "        # Collect all .npy file paths\n",
        "        for root, _, files in os.walk(base_folder):\n",
        "            for file in files:\n",
        "                if file.endswith(\".npy\"):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    self.file_paths.append(file_path)\n",
        "\n",
        "        # Define a helper function to load a label from a file\n",
        "        def load_label(file_path):\n",
        "            data = np.load(file_path, allow_pickle=True).item()\n",
        "            return data['label']\n",
        "\n",
        "        # Load labels in parallel using ThreadPoolExecutor\n",
        "        with ThreadPoolExecutor(max_workers=12) as executor:\n",
        "            self.labels = list(executor.map(load_label, self.file_paths))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        data = np.load(file_path, allow_pickle=True).item()\n",
        "        if data['array'].ndim == 0:\n",
        "            raise ValueError(f\"Zero-dimensional array in file: {file_path}\")\n",
        "        grayscale_sequence = np.expand_dims(data['array'][..., 0], axis=-1)\n",
        "        optical_flow_sequence = data['array'][..., 1:3]\n",
        "\n",
        "        combined_sequence = np.concatenate([grayscale_sequence, optical_flow_sequence], axis=-1)\n",
        "        combined_sequence = np.transpose(combined_sequence, (3, 0, 1, 2))\n",
        "\n",
        "        label = int(data['label'])\n",
        "        if label in range(1, 6):\n",
        "            label = 1\n",
        "        else:\n",
        "            label = 0\n",
        "\n",
        "        return torch.tensor(combined_sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n"
      ],
      "metadata": {
        "id": "73DoIuyjK-9V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = '/content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/fine-tuning.pth'\n",
        "model = FallDetectionCNN().to(device)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "CCYTpCcVJNGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcffab4-044a-4758-c91f-37e9c5e76d1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FallDetectionCNN(\n",
              "  (conv1): Conv3d(2, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (bn4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (global_avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
              "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (dropout1): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "learning_rate = 0.000001\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-5)"
      ],
      "metadata": {
        "id": "TERQIJ-FKA1q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "version_number = 'v10.10'\n",
        "# Data root directory (\n",
        "    # To get this, Run the Generate.ipynb to reconstruct the dataset\n",
        "    # Run the Chunk-Prepaireo-for-Fine-Tuning-04.ipynb to rename the directories Then process to npy files and finaly label them\n",
        "data_root = f'/content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/vae_model_{version_number}/videos/processed-npy-files'\n",
        "\n",
        "# Create the dataset\n",
        "dataset = OpticalFlow3DDataset(data_root)\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "id": "2u9-3qEURCPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479651c0-64de-4de4-e4f3-128f92023f05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "train_val_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "train_dataset, val_dataset = train_test_split(train_val_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoaders\n",
        "dataloader_train = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "dataloader_val = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "dataloader_test = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "vo1CbtuZXk0w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "binary_labels = [1 if int(label) in range(1, 6) else 0 for label in dataset.labels]\n",
        "\n",
        "# Count the each label.\n",
        "label_counts = Counter(binary_labels)\n",
        "print(\"Label balance:\", label_counts)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")\n",
        "print(f\"Shape of a single training sample: {train_dataset[0][0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4HU9ExTOE20",
        "outputId": "17113bc6-6974-41de-ebdf-da20c5ded50c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label balance: Counter({1: 621})\n",
            "Number of training samples: 396\n",
            "Number of validation samples: 100\n",
            "Number of test samples: 125\n",
            "Shape of a single training sample: torch.Size([2, 7, 51, 38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "binary_labels = np.array([1 if int(label) in range(1, 6) else 0 for label in dataset.labels])\n",
        "label_counts = Counter(binary_labels)\n",
        "print(\"Overall Label balance:\", label_counts)\n",
        "\n",
        "total_samples = len(binary_labels)\n",
        "class_weights = {cls: total_samples/count for cls, count in label_counts.items()}\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "train_labels = [1 if int(dataset.labels[i]) in range(1, 6) else 0 for i in train_dataset.indices] \\\n",
        "    if hasattr(train_dataset, 'indices') else [1 if int(dataset.labels[i]) in range(1, 6) else 0 for i in range(len(train_dataset))]\n",
        "\n",
        "sample_weights = [class_weights[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_dataset), replacement=True)\n",
        "\n",
        "dataloader_train = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
        "\n",
        "print(f\"Number of training samples (after balancing): {len(train_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfF7ReqzDFkp",
        "outputId": "db88f24b-6d1a-43ad-dfe3-f8cf6bd9ecdf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Label balance: Counter({np.int64(1): 621})\n",
            "Class weights: {np.int64(1): 1.0}\n",
            "Number of training samples (after balancing): 396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels in dataloader:\n",
        "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    accuracy, precision, recall, specificity, f1 = compute_metrics(all_labels, all_preds)\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, specificity, f1\n",
        "\n",
        "def compute_metrics(true_labels, predictions):\n",
        "    labels = sorted(list(set(true_labels) | set(predictions)))\n",
        "\n",
        "    # Single-class case (Falls only)\n",
        "    if len(labels) < 2:\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision = 1.0 if all(p == labels[0] for p in predictions) else 0.0\n",
        "        recall = 1.0\n",
        "        specificity = 1.0\n",
        "        f1 = f1_score(true_labels, predictions, zero_division=1)\n",
        "        return accuracy, precision, recall, specificity, f1\n",
        "\n",
        "    # Two-class case (Falls and non-falls)\n",
        "    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=1)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=1)\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 1.0\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=1)\n",
        "    return accuracy, precision, recall, specificity, f1\n"
      ],
      "metadata": {
        "id": "xEnG7KZ1YAvK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation before Fine tuning"
      ],
      "metadata": {
        "id": "E7HeBYFZMx4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_npy_files = '/content/drive/MyDrive/Herts - BSc /3rd Year/FYP/Workflow/Final-WorkFlow/test_set_npy/'\n",
        "test_dataset = OpticalFlow3DDataset(test_npy_files)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "7juBzAdVcoOb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Because the training dataset contains only falls (One class) it's hard to get meaningfull evaluation results"
      ],
      "metadata": {
        "id": "6UfAgczgGJxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "initial_loss, initial_acc, initial_prec, initial_rec, initial_spec, initial_f1 = evaluate_model(model, test_loader, criterion, device)"
      ],
      "metadata": {
        "id": "l2YZzbt-X9R_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial Evaluation:\")\n",
        "print(f\"Loss: {initial_loss:.4f}\")\n",
        "print(f\"Accuracy: {initial_acc:.4f}\")\n",
        "print(f\"Precision: {initial_prec:.4f}\")\n",
        "print(f\"Recall: {initial_rec:.4f}\")\n",
        "print(f\"Specificity: {initial_spec:.4f}\")\n",
        "print(f\"F1-Score: {initial_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCgkjmfm3WXU",
        "outputId": "eb251346-68ed-4094-f260-f18a366347db"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Evaluation:\n",
            "Loss: 6.6087\n",
            "Accuracy: 0.0400\n",
            "Precision: 1.0000\n",
            "Recall: 0.0400\n",
            "Specificity: 1.0000\n",
            "F1-Score: 0.0769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "31Y0GzK7F1nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_epochs = 10\n",
        "fine_tune_lr = 0.000001\n",
        "fine_tune_weight_decay = 1e-5\n",
        "\n",
        "model_finetune = FallDetectionCNN().to(device)\n",
        "model_finetune.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "optimizer = optim.Adam(model_finetune.parameters(), lr=fine_tune_lr, weight_decay=fine_tune_weight_decay)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(fine_tune_epochs):\n",
        "    model_finetune.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in dataloader_train:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_finetune(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    train_losses.append(running_loss / len(dataloader_train))\n",
        "\n",
        "    # Validation\n",
        "    model_finetune.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader_val:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model_finetune(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    val_losses.append(val_loss / len(dataloader_val))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{fine_tune_epochs} - Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQM0r0KAZPgM",
        "outputId": "c90213ba-1813-41c4-c3fa-b0bba67fcc9a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 1.1418 | Val Loss: 1.3709\n",
            "Epoch 2/10 - Train Loss: 0.8865 | Val Loss: 0.7930\n",
            "Epoch 3/10 - Train Loss: 0.6969 | Val Loss: 0.7560\n",
            "Epoch 4/10 - Train Loss: 0.6415 | Val Loss: 0.5210\n",
            "Epoch 5/10 - Train Loss: 0.5559 | Val Loss: 0.5477\n",
            "Epoch 6/10 - Train Loss: 0.3989 | Val Loss: 0.4095\n",
            "Epoch 7/10 - Train Loss: 0.3834 | Val Loss: 0.4461\n",
            "Epoch 8/10 - Train Loss: 0.2829 | Val Loss: 0.3359\n",
            "Epoch 9/10 - Train Loss: 0.2869 | Val Loss: 0.3316\n",
            "Epoch 10/10 - Train Loss: 0.2286 | Val Loss: 0.2558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-evaluate"
      ],
      "metadata": {
        "id": "ocVnz1eZF7E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_loss, final_acc, final_prec, final_rec, final_spec, final_f1 = evaluate_model(model_finetune, test_loader, criterion, device)\n",
        "print(\"\\nRe-evaluation After Fine-tuning:\")\n",
        "print(f\"Loss: {final_loss:.4f}\")\n",
        "print(f\"Accuracy: {final_acc:.4f}\")\n",
        "print(f\"Precision: {final_prec:.4f}\")\n",
        "print(f\"Recall: {final_rec:.4f}\")\n",
        "print(f\"Specificity: {final_spec:.4f}\")\n",
        "print(f\"F1-Score: {final_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umXFKgJ2Zxvk",
        "outputId": "e7a9c181-aa6a-427e-e51d-ecf56650682c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Re-evaluation After Fine-tuning:\n",
            "Loss: 23.4968\n",
            "Accuracy: 0.3358\n",
            "Precision: 0.2115\n",
            "Recall: 0.1833\n",
            "Specificity: 0.4570\n",
            "F1-Score: 0.1964\n"
          ]
        }
      ]
    }
  ]
}