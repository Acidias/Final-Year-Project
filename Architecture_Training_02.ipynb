{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VAE Model Architecture, Utils and Training Scripts\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tEt8Ly0OLKDh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O25OyD-9z19H"
      },
      "source": [
        "# Imports and drive connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GhJhCWgu9INw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278e1347-2f6a-4216-cfd0-e8646d8a0833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import imageio\n",
        "import inspect\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import LayerNorm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "rr_CNx9-Lis3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "version = \"v10.02\"\n",
        "save_dir = f\"/content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/vae_model_{version}\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "wsfW63s0OikC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"{save_dir}/model_config.json\"\n",
        "{\n",
        "  \"latent_dim\": 512,\n",
        "  \"batch_size\": 8,\n",
        "  \"num_heads\": 4,\n",
        "  \"num_epochs\": 50,\n",
        "  \"learning_rate\": 0.00005,\n",
        "  \"weight_decay\": 1e-5,\n",
        "  \"lr_scheduler\": \"cosine\",\n",
        "  \"early_stop_patience\": 5,\n",
        "  \"beta_start\": 1.0,\n",
        "  \"beta_end\": 0.1,\n",
        "  \"warmup_epochs\": 20,\n",
        "  \"anneal_rate\": -0.08,\n",
        "  \"train_ratio\": 0.7,\n",
        "  \"val_ratio\": 0.2,\n",
        "  \"test_ratio\": 0.1,\n",
        "  \"hidden_dims\": [32, 64, 128, 256, 512]\n",
        "}"
      ],
      "metadata": {
        "id": "SMHCMQVsR2aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e8cd22-8839-4550-f791-c102f9a00eed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/vae_model_v10.02/model_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToxUD6Md0kIi"
      },
      "source": [
        "# VideoDataset Class\n",
        "\n",
        "\n",
        "A helper class that designed to handle video data stored as a NumPy array. PyTorch expects float32 format and organised in (C, T, H, W) order. This class converts the dataset into that format, to make sure its ready for PyTortch.\n",
        "\n",
        "*   **T** - Number of frames in the video (time).\n",
        "\n",
        "*   **H** - Height of each frame (pixels).\n",
        "\n",
        "*   **W** - Width of each frame (pixels).\n",
        "\n",
        "*   **C** - Number of color channels (3 = RGB).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zXPU585i9Dah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ed8dfc-8c20-487f-9332-19820c2bd0e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/vae_model_v10.02/VideoDataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"{save_dir}/VideoDataset.py\"\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_array):\n",
        "        super().__init__()\n",
        "        self.data = video_array\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a single video in NumPy form\n",
        "        vid_np = self.data[idx]\n",
        "        # Convert from NumPy to float32 PyTorch tensor\n",
        "        vid_tensor = torch.from_numpy(vid_np).float()\n",
        "        # Rearrange dimensions from (T, H, W, C) to (C, T, H, W)\n",
        "        vid_tensor = vid_tensor.permute(3, 0, 1, 2)\n",
        "        return vid_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VideoVAE Model Architecture\n",
        "\n",
        "The VideoVAE class implements a 3D Convolutional Variational Autoencoder (VAE) for video data. The input is a short video clip with shape (batch_size, 3, 9, 128, 128) representing RGB videos with 9 frames and spatial resolution of 128x128.\n",
        "\n",
        "The encoder has five 3D convolutional blocks with increasing channel dimension: [32, 64, 128, 256, 512], and gradually reduces the spatial dimensions while preserving the temporal dimension. It flattens the encoded features and uses two fully connected layers to produce the mean (mu) and log-variance (log_var) vectors of the latent space.\n",
        "\n",
        "The decoder mirrors the encoder using 3D transposed convolutions to reconstruct the input video.\n",
        "\n",
        "* 5 Encoder Blocks + FC layers for latent distribution\n",
        "\n",
        "* 5 Decoder Blocks + Final Reconstruction Layer\n",
        "\n",
        "* Latent sampling with the reparameterisation trick\n",
        "\n",
        "Four skip connections are kept between the Encoder and Decoder blocks, but they are turned OFF!\n",
        "\n",
        "```\n",
        "reconstruction = self.decode(z, skips=None)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "py1BIAQawu_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0Dz7o-TR9wju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7836f7f7-7553-49d2-82f7-9470c156c59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/vae_model_v10.02/model_architecture.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"{save_dir}/model_architecture.py\"\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Any\n",
        "\n",
        "class VideoVAE(nn.Module):\n",
        "    def __init__(self, in_channels: int, latent_dim: int, hidden_dims: List = None, **kwargs) -> None:\n",
        "        super(VideoVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [32, 64, 128, 256, 512]\n",
        "        self.hidden_dims = hidden_dims\n",
        "\n",
        "        # Encoder Blocks\n",
        "        self.encoder_block1 = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, hidden_dims[0], kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1)),\n",
        "            # Hidden dimension: 32, so the channel dimension from 3 (RGB) is increased to 32 (Fetaures like, shapes, lines, etc.)\n",
        "            # The time dimension is kept to 9, but the spatial dimensions are divided by two (stride=(1,2,2))\n",
        "            # The Padding is necessary to keep the expected output shape by compensating for the kernel size.\n",
        "            # Input: (batch_size, 3, 9, 128, 128) -> Output: (batch_size, 32, 9, 64, 64)\n",
        "            nn.BatchNorm3d(hidden_dims[0]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.encoder_block2 = nn.Sequential(\n",
        "            nn.Conv3d(hidden_dims[0], hidden_dims[1], kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1)),\n",
        "            # Current hidden dimension: 64\n",
        "            # Input: (batch_size, 32, 9, 64, 64) -> Output: (batch_size, 64, 9, 32, 32)\n",
        "            nn.BatchNorm3d(hidden_dims[1]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.encoder_block3 = nn.Sequential(\n",
        "            nn.Conv3d(hidden_dims[1], hidden_dims[2], kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1)),\n",
        "            # Current hidden dimension: 128\n",
        "            # Input: (batch_size, 64, 9, 32, 32) -> Output: (batch_size, 128, 9, 16, 16)\n",
        "            nn.BatchNorm3d(hidden_dims[2]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.encoder_block4 = nn.Sequential(\n",
        "            nn.Conv3d(hidden_dims[2], hidden_dims[3], kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1)),\n",
        "            # Current hidden dimension: 256\n",
        "            # Input: (batch_size, 128, 9, 16, 16) -> Output: (batch_size, 256, 9, 8, 8)\n",
        "            nn.BatchNorm3d(hidden_dims[3]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.encoder_block5 = nn.Sequential(\n",
        "            nn.Conv3d(hidden_dims[3], hidden_dims[4], kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1)),\n",
        "            # Current hidden dimension: 512\n",
        "            # Input: (batch_size, 256, 9, 8, 8) -> Output: (batch_size, 512, 9, 4, 4)\n",
        "            nn.BatchNorm3d(hidden_dims[4]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        # After the Last layer: (batch_size, 512, 9, 4, 4)\n",
        "        self.encoder_out_shape = (hidden_dims[-1], 9, 4, 4)\n",
        "        # Flatten dimension is: 512x9x4x4 = 73728\n",
        "        self.flatten_dim = hidden_dims[-1] * 9 * 4 * 4\n",
        "\n",
        "        #\n",
        "        self.fc_mu = nn.Linear(self.flatten_dim, latent_dim)\n",
        "        self.fc_var = nn.Linear(self.flatten_dim, latent_dim)\n",
        "\n",
        "        # Decoder: Fully connected layer to reshape latent vector\n",
        "        self.decoder_input = nn.Linear(latent_dim, self.flatten_dim)\n",
        "        # Input: (batch_size, latent_dim) -> Output: (batch_size, 73728)\n",
        "        # Expands the latent vector back to a flattened shape, then reshaped to (batch_size, 512, 9, 4, 4) for decoding.\n",
        "\n",
        "        # Decoder Blocks\n",
        "        hidden_dims_rev = hidden_dims[::-1]  # Revers to: [512, 256, 128, 64, 32]\n",
        "        self.decoder_block1 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(hidden_dims_rev[0],\n",
        "                               hidden_dims_rev[1],\n",
        "                               kernel_size=(3,3,3),\n",
        "                               stride=(1,2,2),\n",
        "                               padding=(1,1,1),\n",
        "                               output_padding=(0,1,1)),\n",
        "            # Current hidden dimension: 256, reduces channels from 512 to 256 while upsampling spatially.\n",
        "            # Stride=(1,2,2) keeps time at 9 but doubles spatial dimensions; output_padding adjusts for exact size.\n",
        "            # Input: (batch_size, 512, 9, 4, 4) -> Output: (batch_size, 256, 9, 8, 8)\n",
        "            nn.BatchNorm3d(hidden_dims_rev[1]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.decoder_block2 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(hidden_dims_rev[1],\n",
        "                               hidden_dims_rev[2],\n",
        "                               kernel_size=(3,3,3),\n",
        "                               stride=(1,2,2),\n",
        "                               padding=(1,1,1),\n",
        "                               output_padding=(0,1,1)),\n",
        "            # Current hidden dimension: 128\n",
        "            # Input: (batch_size, 256, 9, 8, 8) -> Output: (batch_size, 128, 9, 16, 16)\n",
        "            nn.BatchNorm3d(hidden_dims_rev[2]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.decoder_block3 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(hidden_dims_rev[2],\n",
        "                               hidden_dims_rev[3],\n",
        "                               kernel_size=(3,3,3),\n",
        "                               stride=(1,2,2),\n",
        "                               padding=(1,1,1),\n",
        "                               output_padding=(0,1,1)),\n",
        "            # Current hidden dimension: 64\n",
        "            # Input: (batch_size, 128, 9, 16, 16) -> Output: (batch_size, 64, 9, 32, 32)\n",
        "            nn.BatchNorm3d(hidden_dims_rev[3]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.decoder_block4 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(hidden_dims_rev[3],\n",
        "                               hidden_dims_rev[4],\n",
        "                               kernel_size=(3,3,3),\n",
        "                               stride=(1,2,2),\n",
        "                               padding=(1,1,1),\n",
        "                               output_padding=(0,1,1)),\n",
        "            # Current hidden dimension: 32\n",
        "            # Input: (batch_size, 64, 9, 32, 32) -> Output: (batch_size, 32, 9, 64, 64)\n",
        "            nn.BatchNorm3d(hidden_dims_rev[4]),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.ConvTranspose3d(hidden_dims_rev[-1],\n",
        "                               hidden_dims_rev[-1],\n",
        "                               kernel_size=(3,3,3),\n",
        "                               stride=(1,2,2),\n",
        "                               padding=(1,1,1),\n",
        "                               output_padding=(0,1,1)),\n",
        "            # Current hidden dimension: 32\n",
        "            # Input: (batch_size, 32, 9, 64, 64) -> Output: (batch_size, 32, 9, 128, 128)\n",
        "            nn.BatchNorm3d(hidden_dims_rev[-1]),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv3d(hidden_dims_rev[-1], out_channels=3,\n",
        "                      kernel_size=(3,3,3), padding=(1,1,1)),\n",
        "            # Reduces the channels from 32 to 3\n",
        "            # Input: (batch_size, 32, 9, 128, 128) -> Output: (batch_size, 3, 9, 128, 128)\n",
        "            nn.Sigmoid()\n",
        "            # Normalises the output pixels to [0,1] (RGB)\n",
        "        )\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> List[Tensor]:\n",
        "        # Encodes the input video into a latent representation and collects skip connections.\n",
        "        # Input: (batch_size, 3, 9, 128, 128)\n",
        "        e1 = self.encoder_block1(x)   # Shape: (batch_size, 32, 9, 64, 64)\n",
        "        e2 = self.encoder_block2(e1)  # Shape: (batch_size, 64, 9, 32, 32)\n",
        "        e3 = self.encoder_block3(e2)  # Shape: (batch_size, 128, 9, 16, 16)\n",
        "        e4 = self.encoder_block4(e3)  # Shape: (batch_size, 256, 9, 8, 8)\n",
        "        e5 = self.encoder_block5(e4)  # Shape: (batch_size, 512, 9, 4, 4)\n",
        "        x_flat = torch.flatten(e5, start_dim=1)  # Shape: (batch_size, 73728)\n",
        "        # Compute latent distribution parameters\n",
        "        mu = self.fc_mu(x_flat)   # Shape: (batch_size, latent_dim)\n",
        "        log_var = self.fc_var(x_flat)  # Shape: (batch_size, latent_dim)\n",
        "        # Return mean, log variance, and skip connections for decoding\n",
        "        return [mu, log_var, (e1, e2, e3, e4)]\n",
        "\n",
        "    def decode(self, z: torch.Tensor, skips: tuple = None) -> torch.Tensor:\n",
        "        # Decodes the latent vector back to a video, using skip connections from the encoder.\n",
        "        # Input z: (batch_size, latent_dim), skips: tuple of 4 tensors from encoder\n",
        "        x = self.decoder_input(z)  # Shape: (batch_size, 73728)\n",
        "        x = x.view(-1, *self.encoder_out_shape)  # Reshape to: (batch_size, 512, 9, 4, 4)\n",
        "        # Decoder block 1: upsample and add skip connection from encoder_block4\n",
        "        d1 = self.decoder_block1(x)  # Shape: (batch_size, 256, 9, 8, 8)\n",
        "        if skips is not None:\n",
        "          d1 = d1 + skips[3]  # Add skip: (batch_size, 256, 9, 8, 8)\n",
        "        # Decoder block 2: upsample and add skip from encoder_block3\n",
        "        d2 = self.decoder_block2(d1)  # Shape: (batch_size, 128, 9, 16, 16)\n",
        "        if skips is not None:\n",
        "          d2 = d2 + skips[2]  # Add skip: (batch_size, 128, 9, 16, 16)\n",
        "        # Decoder block 3: upsample and add skip from encoder_block2\n",
        "        d3 = self.decoder_block3(d2)  # Shape: (batch_size, 64, 9, 32, 32)\n",
        "        if skips is not None:\n",
        "          d3 = d3 + skips[1]  # Add skip: (batch_size, 64, 9, 32, 32)\n",
        "        # Decoder block 4: upsample and add skip from encoder_block1\n",
        "        d4 = self.decoder_block4(d3)  # Shape: (batch_size, 32, 9, 64, 64)\n",
        "        if skips is not None:\n",
        "          d4 = d4 + skips[0]  # Add skip: (batch_size, 32, 9, 64, 64)\n",
        "        # Reconstruction to original video shape\n",
        "        x_recon = self.final_layer(d4)  # Shape: (batch_size, 3, 9, 128, 128)\n",
        "        return x_recon\n",
        "\n",
        "    def reparameterise(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        # The Reparameterisation trick for sampling from the latent distribution by adding noise (eps)\n",
        "        # Input: mu: (batch_size, latent_dim), logvar: (batch_size, latent_dim)\n",
        "        std = torch.exp(0.5 * logvar)  # Compute standard deviation\n",
        "        eps = torch.randn_like(std)  # noise\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, x: torch.Tensor, **kwargs) -> List[Tensor]:\n",
        "        # Full forward pass: encode, reparameterize, and decode.\n",
        "        # Input: (batch_size, 3, 9, 128, 128)\n",
        "        mu, log_var, skips = self.encode(x) # encode to latent space and get skips\n",
        "        z = self.reparameterise(mu, log_var) # sample from latent distribution using reparameterisation\n",
        "        reconstruction = self.decode(z, skips=None) # Skips Turned Off for reconstruction\n",
        "        return [reconstruction, mu, log_var]\n",
        "\n",
        "    def sample(self, num_samples: int, device: int, **kwargs) -> torch.Tensor:\n",
        "        # Generate new video samples from the latent space without skip connections.\n",
        "        z = torch.randn(num_samples, self.latent_dim).to(device)  # Shape: (num_samples, latent_dim)\n",
        "        samples = self.decode(z)  # Decode without skips\n",
        "        return samples\n",
        "\n",
        "    def generate(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "        # Generate reconstructions from input data (essentially a forward pass returning only the reconstruction).\n",
        "        # Input: (batch_size, 3, 9, 128, 128)\n",
        "        return self.forward(x)[0]  # Return only the reconstructed video: (batch_size, 3, 9, 128, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions for Traning and Evaluation\n",
        "\n",
        "* **get_data_loaders:**\n",
        "Splits the dataset into training, validation, and test sets, then returns PyTorch DataLoader objects for each split\n",
        "\n",
        "* **save_reconstruction_plots**\n",
        "Visualises and saves a comparison of original and reconstructed frames\n",
        "\n",
        "* **save_sample_plots**\n",
        "Generates and saves samples from random latent vectors, showing the middle frame of each generated video\n",
        "\n",
        "* **save_latent_distribution**\n",
        "Extracts latent means from the model, applies PCA for dimensionality reduction, clusters them using KMeans, and saves a scatter plot to visualise the latent space\n",
        "\n",
        "* **visualise_cluster_samples**\n",
        "Groups encoded videos into 2 clusters.\n",
        "\n",
        "* **plot_loss_curves**\n",
        "Plots and saves training and validation loss curves over epochs.\n",
        "\n",
        "* **save_tsne_latent_visualisation**\n",
        "Applies t-SNE to project high-dimensional latent vectors into 2D and visualises clusters.\n",
        "\n",
        "* **natural_sort_key**\n",
        "Provides a key for naturally sorting filenames that contain both letters and numbers.\n",
        "\n",
        "* **create_gif_from_folder**\n",
        "Combines a sequence of image files from a folder into an animated GIF.\n",
        "\n",
        "* **log_print**\n",
        "Prints and logs messages to a file simultaneously.\n",
        "\n",
        "* **compute_loss**\n",
        "Calculates the total VAE loss, including reconstruction loss (MSE) and KL divergence loss, with a beta scaling.\n",
        "\n"
      ],
      "metadata": {
        "id": "W8G5Q24ayczm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"{save_dir}/utils.py\"\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import torch\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "from VideoDataset import VideoDataset\n",
        "\n",
        "def get_data_loaders(video_array, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, batch_size=8, shuffle=True, num_workers=4, pin_memory=True):\n",
        "    \"\"\"Splits video array into train, validation, and test sets. Returns DataLoaders.\"\"\"\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n",
        "\n",
        "    dataset = VideoDataset(video_array)\n",
        "    dataset_size = len(dataset)\n",
        "    train_size = int(train_ratio * dataset_size)\n",
        "    val_size = int(val_ratio * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(42)\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def save_reconstruction_plots(model, data_loader, epoch, device, save_dir):\n",
        "    \"\"\"Saves plot comparing original and reconstructed frames from a validation video.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(data_loader))\n",
        "        batch = batch.to(device)\n",
        "        recon_batch, _, _ = model(batch)\n",
        "        sample = batch[0:1]  # Shape: (1, 3, T, H, W)\n",
        "        recon_sample = recon_batch[0:1]\n",
        "\n",
        "        T = sample.shape[2]\n",
        "        if T >= 3:\n",
        "            indices = [0, T // 2, T - 1]\n",
        "        else:\n",
        "            indices = list(range(T))\n",
        "\n",
        "        n_frames = len(indices)\n",
        "        fig, axes = plt.subplots(2, n_frames, figsize=(4 * n_frames, 8))\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            # Convert frame from (C, H, W) to (H, W, C)\n",
        "            orig_frame = sample[0, :, idx, :, :].cpu().permute(1, 2, 0).numpy()\n",
        "            recon_frame = recon_sample[0, :, idx, :, :].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Scale images from [-1, 1] to [0, 1] if needed (uncomment below)\n",
        "            # orig_frame = (orig_frame + 1) / 2\n",
        "            # recon_frame = (recon_frame + 1) / 2\n",
        "\n",
        "            axes[0, i].imshow(orig_frame)\n",
        "            axes[0, i].set_title(f\"Original\\nFrame {idx}\")\n",
        "            axes[0, i].axis(\"off\")\n",
        "\n",
        "            axes[1, i].imshow(recon_frame)\n",
        "            axes[1, i].set_title(f\"Reconstruction\\nFrame {idx}\")\n",
        "            axes[1, i].axis(\"off\")\n",
        "\n",
        "        plt.suptitle(f\"Reconstruction Comparison - Epoch {epoch}\")\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "        recon_plot_path = os.path.join(save_dir, f\"recon_epoch_{epoch}.png\")\n",
        "        plt.savefig(recon_plot_path)\n",
        "        plt.close()\n",
        "\n",
        "def save_sample_plots(model, epoch, device, save_dir, num_samples=4):\n",
        "    \"\"\"Saves plot of middle frames from videos generated by random latent codes.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        samples = model.sample(num_samples=num_samples, device=device)  # Shape: (num_samples, 3, T, H, W)\n",
        "        T = samples.shape[2]\n",
        "        frame_idx = T // 2  # Select middle frame\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_samples, figsize=(4 * num_samples, 4))\n",
        "        for i in range(num_samples):\n",
        "            sample_frame = samples[i, :, frame_idx, :, :].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Scale image from [-1, 1] to [0, 1] if needed (uncomment below)\n",
        "            # sample_frame = (sample_frame + 1) / 2\n",
        "\n",
        "            axes[i].imshow(sample_frame)\n",
        "            axes[i].set_title(f\"Sample {i}\")\n",
        "            axes[i].axis(\"off\")\n",
        "\n",
        "        plt.suptitle(f\"Random Samples - Epoch {epoch}\")\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "        sample_plot_path = os.path.join(save_dir, f\"samples_epoch_{epoch}.png\")\n",
        "        plt.savefig(sample_plot_path)\n",
        "        plt.close()\n",
        "\n",
        "def save_latent_distribution(model, data_loader, epoch, device, save_dir, num_samples=100, num_clusters=5):\n",
        "    \"\"\"Saves plot of latent distribution using PCA and KMeans clustering.\"\"\"\n",
        "    model.eval()\n",
        "    latent_means = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = batch.to(device)\n",
        "            _, mu, _ = model(batch)\n",
        "            latent_means.append(mu.cpu())\n",
        "            if len(latent_means) * batch.size(0) >= num_samples:\n",
        "                break\n",
        "        latent_means = torch.cat(latent_means, dim=0)[:num_samples].numpy()\n",
        "\n",
        "        # Cluster latent means\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(latent_means)\n",
        "        cluster_labels = kmeans.labels_\n",
        "\n",
        "        # Reduce to 2D with PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        latent_2d = pca.fit_transform(latent_means)\n",
        "\n",
        "        # Plot with cluster colors\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=cluster_labels, alpha=0.7, cmap='viridis')\n",
        "        plt.colorbar(scatter, label='Cluster Label')\n",
        "        plt.xlabel(\"PC1\")\n",
        "        plt.ylabel(\"PC2\")\n",
        "        plt.title(f\"Latent Distribution (PCA) - Epoch {epoch}\")\n",
        "        latent_plot_path = os.path.join(save_dir, f\"latent_distribution_epoch_{epoch}.png\")\n",
        "        plt.savefig(latent_plot_path)\n",
        "        plt.close()\n",
        "\n",
        "def visualise_cluster_samples(model, data_loader, device, save_dir, epoch, num_samples=100, num_samples_per_cluster=3):\n",
        "    \"\"\"Saves plot of middle frames from samples clustered into 2 groups.\"\"\"\n",
        "    model.eval()\n",
        "    latent_codes = []\n",
        "    videos = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = batch.to(device)\n",
        "            _, mu, _ = model(batch)\n",
        "            latent_codes.append(mu.cpu())\n",
        "            videos.append(batch.cpu())\n",
        "            if len(torch.cat(latent_codes, dim=0)) >= num_samples:\n",
        "                break\n",
        "\n",
        "    latent_codes = torch.cat(latent_codes, dim=0)[:num_samples].numpy()\n",
        "    videos = torch.cat(videos, dim=0)[:num_samples]  # Shape: (num_samples, C, T, H, W)\n",
        "\n",
        "    # Cluster into 2 groups\n",
        "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(latent_codes)\n",
        "\n",
        "    # Collect sample indices per cluster\n",
        "    clusters = {0: [], 1: []}\n",
        "    for idx, label in enumerate(cluster_labels):\n",
        "        if len(clusters[label]) < num_samples_per_cluster:\n",
        "            clusters[label].append(idx)\n",
        "\n",
        "    # Plot middle frame for each sample\n",
        "    fig, axes = plt.subplots(2, num_samples_per_cluster, figsize=(4 * num_samples_per_cluster, 8))\n",
        "    for cluster_idx, indices in clusters.items():\n",
        "        for i, sample_idx in enumerate(indices):\n",
        "            video = videos[sample_idx]\n",
        "            T = video.shape[1]\n",
        "            mid_frame = video[:, T // 2, :, :].permute(1, 2, 0).numpy()\n",
        "            axes[cluster_idx, i].imshow(mid_frame)\n",
        "            axes[cluster_idx, i].set_title(f\"Cluster {cluster_idx} - Sample {i+1}\")\n",
        "            axes[cluster_idx, i].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Representative Samples from Each Cluster\")\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_path = os.path.join(save_dir, f\"clusters_{epoch}.png\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses, train_rec_losses, val_rec_losses, train_kl_losses, val_kl_losses, save_dir):\n",
        "    \"\"\"Saves plots of total, reconstruction, and KL losses for train and validation.\"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Create figure with increased width for wider plots\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(24, 5))\n",
        "\n",
        "    # Total loss plot\n",
        "    axs[0].plot(epochs, train_losses, label=\"Train\", marker='o')\n",
        "    axs[0].plot(epochs, val_losses, label=\"Val\", marker='o')\n",
        "    axs[0].set_title(\"Total Loss\")\n",
        "    axs[0].set_xlabel(\"Epoch\")\n",
        "    axs[0].set_ylabel(\"Loss\")\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Reconstruction loss plot\n",
        "    axs[1].plot(epochs, train_rec_losses, label=\"Train Rec\", marker='o')\n",
        "    axs[1].plot(epochs, val_rec_losses, label=\"Val Rec\", marker='o')\n",
        "    axs[1].set_title(\"Reconstruction Loss\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Loss\")\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    # KL loss plot\n",
        "    axs[2].plot(epochs, train_kl_losses, label=\"Train KL\", marker='o')\n",
        "    axs[2].plot(epochs, val_kl_losses, label=\"Val KL\", marker='o')\n",
        "    axs[2].set_title(\"KL Loss\")\n",
        "    axs[2].set_xlabel(\"Epoch\")\n",
        "    axs[2].set_ylabel(\"Loss\")\n",
        "    axs[2].legend()\n",
        "    axs[2].grid(True)\n",
        "\n",
        "    # Adjust layout to prevent overlap\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(save_dir, \"loss_curves.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "def save_tsne_latent_visualisation(model, data_loader, save_path, num_samples=100):\n",
        "    \"\"\"Saves t-SNE plot of latent codes clustered into 2 groups.\"\"\"\n",
        "    model.eval()\n",
        "    latent_codes = []\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = batch.to(next(model.parameters()).device)\n",
        "            _, mu, _ = model(batch)\n",
        "            latent_codes.append(mu.cpu())\n",
        "            count += mu.size(0)\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "\n",
        "    latent_codes = torch.cat(latent_codes, dim=0)[:num_samples].numpy()\n",
        "\n",
        "    # Cluster into 2 groups\n",
        "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(latent_codes)\n",
        "\n",
        "    # Reduce to 2D with t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    tsne_results = tsne.fit_transform(latent_codes)\n",
        "\n",
        "    # Plot with cluster colors\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=cluster_labels, cmap='tab10', alpha=0.7)\n",
        "    plt.colorbar(scatter, label='Cluster Label')\n",
        "    plt.title(\"t-SNE of Video Latent Representations\")\n",
        "    plt.xlabel(\"Dimension 1\")\n",
        "    plt.ylabel(\"Dimension 2\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# Sorts file names naturally by splitting numbers and text\n",
        "def natural_sort_key(s):\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
        "\n",
        "def create_gif_from_folder(folder_path, gif_path, pattern=\"*.png\", duration=0.5):\n",
        "    \"\"\"Creates GIF from PNG images in a folder, sorted naturally.\"\"\"\n",
        "    images = []\n",
        "    file_names = sorted(glob.glob(os.path.join(folder_path, pattern)), key=natural_sort_key)\n",
        "    for filename in file_names:\n",
        "        images.append(imageio.imread(filename))\n",
        "    imageio.mimsave(gif_path, images, duration=duration)\n",
        "\n",
        "def log_print(msg, file_obj):\n",
        "    print(msg)\n",
        "    file_obj.write(msg + \"\\n\")\n",
        "    file_obj.flush()\n",
        "\n",
        "def compute_loss(recon_batch, batch, mu, logvar, beta):\n",
        "    rec_loss = F.mse_loss(recon_batch, batch, reduction='sum') / batch.size(0)\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch.size(0)\n",
        "    loss = rec_loss + beta * kl_loss\n",
        "    return loss, rec_loss, kl_loss"
      ],
      "metadata": {
        "id": "sNGSLgUv5uLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999de8ed-0c0d-4290-a917-34e19a831c69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/vae_model_v10.02/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VAE Training Pipeline**\n",
        "This script defines the full training loop for the VideoVAE model. It includes functionality for training, validation, testing, visualisation, and logging.\n",
        "\n",
        "### Training Phase\n",
        "Optimises the model using reconstruction and KL divergence losses with a configurable beta parameter.\n",
        "\n",
        "### Validation Phase\n",
        "Evaluates the model after each epoch and implements early stopping based on validation loss improvements.\n",
        "\n",
        "### Testing Phase\n",
        "Assesses final model performance using the test set after training concludes.\n",
        "\n",
        "\n",
        "### Logging and Visualisation\n",
        "\n",
        "* Reconstruction comparisons\n",
        "* Random sample generations\n",
        "* Latent space PCA plots\n",
        "* t-SNE embeddings\n",
        "* Cluster visualisation\n",
        "\n",
        "### After Training\n",
        "* Loss curves\n",
        "* GIFs for visual progress\n",
        "\n",
        "### Model Checkpoint\n",
        "Automatically saves the best-performing model based on validation loss."
      ],
      "metadata": {
        "id": "W_sFJl9Z0Cxx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UmRrFjo8ByHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58e02fe-8b3b-49d6-c283-dd4fb789ceff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/vae_model_v10.02/train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"{save_dir}/train.py\"\n",
        "import os\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, StepLR\n",
        "\n",
        "from utils import (\n",
        "    save_reconstruction_plots,\n",
        "    save_sample_plots,\n",
        "    save_latent_distribution,\n",
        "    plot_loss_curves,\n",
        "    create_gif_from_folder,\n",
        "    save_tsne_latent_visualisation,\n",
        "    visualise_cluster_samples,\n",
        "    log_print,\n",
        "    compute_loss\n",
        ")\n",
        "\n",
        "def train(model, train_loader, val_loader, test_loader, optimiser, scheduler, config, device, eval_dir=\"eval_plots\"):\n",
        "    recon_dir = os.path.join(eval_dir, \"recon\")\n",
        "    samples_dir = os.path.join(eval_dir, \"samples\")\n",
        "    latent_dir = os.path.join(eval_dir, \"latent\")\n",
        "    best_model_dir = os.path.join(eval_dir, \"best_model\")\n",
        "    loss_dir = os.path.join(eval_dir, \"loss\")\n",
        "    tsne_dir = os.path.join(eval_dir, \"tsne\")\n",
        "    log_dir = os.path.join(eval_dir, \"logs\")\n",
        "    latent_sample_visualisation_dir = os.path.join(eval_dir, \"latent_sample_visualisation\")\n",
        "    for d in [recon_dir, samples_dir, latent_dir, best_model_dir, loss_dir, tsne_dir, latent_sample_visualisation_dir, log_dir]:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    log_file_path = os.path.join(log_dir, \"training_log.txt\")\n",
        "    log_file = open(log_file_path, \"a\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "    patience = config[\"early_stop_patience\"]\n",
        "    beta_start = config[\"beta_start\"]\n",
        "    beta_end = config[\"beta_end\"]\n",
        "    warmup_epochs = config[\"warmup_epochs\"]\n",
        "    anneal_rate = config[\"anneal_rate\"]\n",
        "    num_clusters = 2\n",
        "\n",
        "    train_losses, val_losses, test_losses = [], [], []\n",
        "    train_rec_losses, val_rec_losses, test_rec_losses = [], [], []\n",
        "    train_kl_losses, val_kl_losses, test_kl_losses = [], [], []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epoch_no_improvement = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # ---- Training Phase ----\n",
        "        model.train()\n",
        "        beta = 1.0\n",
        "        train_loss_epoch = train_rec_loss_epoch = train_kl_loss_epoch = 0.0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimiser.zero_grad()\n",
        "            recon_batch, mu, logvar = model(batch)\n",
        "            loss, rec_loss, kl_loss = compute_loss(recon_batch, batch, mu, logvar, beta)\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            train_loss_epoch += loss.item() * batch.size(0)\n",
        "            train_rec_loss_epoch += rec_loss.item() * batch.size(0)\n",
        "            train_kl_loss_epoch += kl_loss.item() * batch.size(0)\n",
        "\n",
        "        train_loss_epoch /= len(train_loader.dataset)\n",
        "        train_rec_loss_epoch /= len(train_loader.dataset)\n",
        "        train_kl_loss_epoch /= len(train_loader.dataset)\n",
        "        train_losses.append(train_loss_epoch)\n",
        "        train_rec_losses.append(train_rec_loss_epoch)\n",
        "        train_kl_losses.append(train_kl_loss_epoch)\n",
        "\n",
        "        # ---- Validation Phase ----\n",
        "        model.eval()\n",
        "        val_loss_epoch = val_rec_loss_epoch = val_kl_loss_epoch = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                recon_batch, mu, logvar = model(batch)\n",
        "                loss, rec_loss, kl_loss = compute_loss(recon_batch, batch, mu, logvar, beta)\n",
        "                val_loss_epoch += loss.item() * batch.size(0)\n",
        "                val_rec_loss_epoch += rec_loss.item() * batch.size(0)\n",
        "                val_kl_loss_epoch += kl_loss.item() * batch.size(0)\n",
        "\n",
        "        val_loss_epoch /= len(val_loader.dataset)\n",
        "        val_rec_loss_epoch /= len(val_loader.dataset)\n",
        "        val_kl_loss_epoch /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss_epoch)\n",
        "        val_rec_losses.append(val_rec_loss_epoch)\n",
        "        val_kl_losses.append(val_kl_loss_epoch)\n",
        "\n",
        "        # ---- Early Stopping ----\n",
        "        if val_loss_epoch < best_val_loss:\n",
        "            best_val_loss = val_loss_epoch\n",
        "            epoch_no_improvement = 0\n",
        "            torch.save(model.state_dict(), os.path.join(best_model_dir, \"best_video_vae.pth\"))\n",
        "        else:\n",
        "            epoch_no_improvement += 1\n",
        "            log_print(f\"Epoch {epoch} - Early stopping counter: {epoch_no_improvement}/{patience}\", log_file)\n",
        "\n",
        "\n",
        "        if epoch_no_improvement >= patience:\n",
        "            log_print(f\"Early stopping triggered at epoch {epoch}\", log_file)\n",
        "            break\n",
        "\n",
        "        # ---- Learning Rate Scheduler Step ----\n",
        "        if scheduler is not None:\n",
        "            if isinstance(scheduler, ReduceLROnPlateau):\n",
        "                scheduler.step(val_loss_epoch)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "        # ---- Evaluation Plots ----\n",
        "        save_reconstruction_plots(model, val_loader, epoch, device, recon_dir)\n",
        "        save_sample_plots(model, epoch, device, samples_dir, num_samples=4)\n",
        "        save_latent_distribution(model, val_loader, epoch, device, latent_dir, num_samples=100, num_clusters=num_clusters)\n",
        "        tsne_save_path = os.path.join(tsne_dir, f\"tsne_epoch_{epoch}.png\")\n",
        "        save_tsne_latent_visualisation(model, val_loader, tsne_save_path)\n",
        "        visualise_cluster_samples(model, val_loader, device, latent_sample_visualisation_dir, epoch, num_samples=100, num_samples_per_cluster=3)\n",
        "\n",
        "        log_print(f\"Epoch {epoch}/{num_epochs} - Beta: {beta:.4f} - \"\n",
        "              f\"Train Loss: {train_loss_epoch:.4f} (Rec: {train_rec_loss_epoch:.4f}, KL: {train_kl_loss_epoch:.4f}) - \"\n",
        "              f\"Val Loss: {val_loss_epoch:.4f} (Rec: {val_rec_loss_epoch:.4f}, KL: {val_kl_loss_epoch:.4f})\", log_file)\n",
        "\n",
        "    # ---- Test Phase ----\n",
        "    model.eval()\n",
        "    test_loss_epoch = test_rec_loss_epoch = test_kl_loss_epoch = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            recon_batch, mu, logvar = model(batch)\n",
        "            loss, rec_loss, kl_loss = compute_loss(recon_batch, batch, mu, logvar, beta)\n",
        "            test_loss_epoch += loss.item() * batch.size(0)\n",
        "            test_rec_loss_epoch += rec_loss.item() * batch.size(0)\n",
        "            test_kl_loss_epoch += kl_loss.item() * batch.size(0)\n",
        "\n",
        "    test_loss_epoch /= len(test_loader.dataset)\n",
        "    test_rec_loss_epoch /= len(test_loader.dataset)\n",
        "    test_kl_loss_epoch /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss_epoch)\n",
        "    test_rec_losses.append(test_rec_loss_epoch)\n",
        "    test_kl_losses.append(test_kl_loss_epoch)\n",
        "\n",
        "\n",
        "    log_print(f\"Test Results - Loss: {test_loss_epoch:.4f} (Rec: {test_rec_loss_epoch:.4f}, KL: {test_kl_loss_epoch:.4f})\", log_file)\n",
        "    log_file.close()\n",
        "    # ---- Save Loss Curves ----\n",
        "    plot_loss_curves(train_losses, val_losses, train_rec_losses, val_rec_losses, train_kl_losses, val_kl_losses, loss_dir)\n",
        "\n",
        "    # ---- Create GIFs ----\n",
        "    create_gif_from_folder(recon_dir, os.path.join(recon_dir, \"reconstruction.gif\"))\n",
        "    create_gif_from_folder(samples_dir, os.path.join(samples_dir, \"samples.gif\"))\n",
        "    create_gif_from_folder(latent_dir, os.path.join(latent_dir, \"latent.gif\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Video Data\n",
        "video_data_path = '/content/drive/MyDrive/Herts - BSc /3rd Year/FYP/processed_data/video_data-9frame-v2.0.npy'\n",
        "video_data = np.load(video_data_path)\n",
        "\n",
        "print(\"Shape of video_data:\", video_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHOK-e_zPmFS",
        "outputId": "3e281986-a0ee-4022-bf7a-b055696d7415"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of video_data: (4588, 9, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution\n",
        "This script loads configuration settings and starts the complete training process for the VideoVAE model. It handles setup, execution, and saving of both the model and the current notebook.\n",
        "\n",
        "### Configuration Loading\n",
        "Reads training parameters from a model_config.json file.\n",
        "\n",
        "### Data Preparation\n",
        "Calls get_data_loaders() to split the dataset into training, validation, and test sets and creates DataLoaders.\n",
        "\n",
        "### Training\n",
        "Calls the train() function with all necessary args. This function handles training, validation, testing, logging, visualisation, and saving the best model.\n",
        "\n",
        "### After training:\n",
        "\n",
        "Saves the final model weights (video_vae_final.pth)\n",
        "\n",
        "Copies the current Jupyter notebook into the output directory (correct version)\n"
      ],
      "metadata": {
        "id": "kxLuA2PM1VfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "os.chdir(f\"{save_dir}\")\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, StepLR\n",
        "\n",
        "from model_architecture import VideoVAE\n",
        "from train import train\n",
        "from utils import get_data_loaders\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config_path = os.path.join(save_dir, \"model_config.json\")\n",
        "    with open(config_path, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    latent_dim = config[\"latent_dim\"]\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    # num_heads = config[\"num_heads\"]\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "    learning_rate = config[\"learning_rate\"]\n",
        "    weight_decay = config[\"weight_decay\"]\n",
        "    lr_scheduler_type = config[\"lr_scheduler\"]\n",
        "    patience = config[\"early_stop_patience\"]\n",
        "    train_ratio = config[\"train_ratio\"]\n",
        "    val_ratio = config[\"val_ratio\"]\n",
        "    test_ratio = config[\"test_ratio\"]\n",
        "    hidden_dims = config[\"hidden_dims\"]\n",
        "\n",
        "    print(f\"Config:\\n latent_dim={latent_dim}\\n learning_rate={learning_rate}\\n lr_scheduler={lr_scheduler_type}\\n batch_size={batch_size}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader, val_loader, test_loader = get_data_loaders(\n",
        "        video_data, train_ratio=train_ratio, val_ratio=val_ratio, test_ratio=test_ratio, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Set Device, Model, Optimiser, and Scheduler\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = VideoVAE(3, latent_dim=latent_dim, hidden_dims=hidden_dims)\n",
        "    optimiser = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    if lr_scheduler_type == \"cosine\":\n",
        "        scheduler = CosineAnnealingLR(optimiser, T_max=num_epochs)\n",
        "    elif lr_scheduler_type == \"step\":\n",
        "        scheduler = StepLR(optimiser, step_size=10, gamma=0.1)\n",
        "    elif lr_scheduler_type == \"plateau\":\n",
        "        scheduler = ReduceLROnPlateau(optimiser, mode='min', factor=0.1, patience=5)\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    # Training\n",
        "    train(model, train_loader, val_loader, test_loader, optimiser, scheduler, config, device, eval_dir=save_dir)\n",
        "\n",
        "    # Save Final Model\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, 'video_vae_final.pth'))\n",
        "    print(\"Training complete. Final model saved as 'video_vae_final.pth'.\")\n",
        "    # Save the ipynb file\n",
        "    current_ipynb_path = '/content/drive/MyDrive/Herts - BSc /3rd Year/FYP/Workflow/Final-WorkFlow/Architecture-Training.ipynb'\n",
        "    destination_ipynb_path = os.path.join(save_dir, \"Architecture-Training.ipynb\")\n",
        "    shutil.copy(current_ipynb_path, destination_ipynb_path)\n",
        "\n",
        "    print(f\"Copied {current_ipynb_path} to {destination_ipynb_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q48aRF9yWMRs",
        "outputId": "86d7e508-0bc8-4c77-c05a-87634eb82c0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config:\n",
            " latent_dim=512\n",
            " learning_rate=5e-05\n",
            " lr_scheduler=cosine\n",
            " batch_size=8\n",
            "Epoch 1/50 - Beta: 1.0000 - Train Loss: 9293.5215 (Rec: 8841.2979, KL: 452.2236) - Val Loss: 6217.2064 (Rec: 5786.7006, KL: 430.5058)\n",
            "Epoch 2/50 - Beta: 1.0000 - Train Loss: 5237.3034 (Rec: 4856.9591, KL: 380.3443) - Val Loss: 4625.8502 (Rec: 4287.2043, KL: 338.6458)\n",
            "Epoch 3/50 - Beta: 1.0000 - Train Loss: 4216.9676 (Rec: 3852.6121, KL: 364.3555) - Val Loss: 3857.4084 (Rec: 3488.2833, KL: 369.1250)\n",
            "Epoch 4/50 - Beta: 1.0000 - Train Loss: 3679.4867 (Rec: 3328.5791, KL: 350.9076) - Val Loss: 3477.0154 (Rec: 3098.4047, KL: 378.6107)\n",
            "Epoch 5/50 - Beta: 1.0000 - Train Loss: 3359.2341 (Rec: 3015.8065, KL: 343.4276) - Val Loss: 3342.4569 (Rec: 2973.4528, KL: 369.0041)\n",
            "Epoch 6/50 - Beta: 1.0000 - Train Loss: 3117.2753 (Rec: 2781.2105, KL: 336.0647) - Val Loss: 3067.8173 (Rec: 2705.9906, KL: 361.8267)\n",
            "Epoch 7/50 - Beta: 1.0000 - Train Loss: 2949.3128 (Rec: 2617.4237, KL: 331.8891) - Val Loss: 2916.5371 (Rec: 2590.2609, KL: 326.2762)\n",
            "Epoch 8/50 - Beta: 1.0000 - Train Loss: 2811.7843 (Rec: 2482.1904, KL: 329.5939) - Val Loss: 2791.0428 (Rec: 2434.7323, KL: 356.3105)\n",
            "Epoch 9/50 - Beta: 1.0000 - Train Loss: 2664.8963 (Rec: 2347.6320, KL: 317.2644) - Val Loss: 2756.6944 (Rec: 2418.4385, KL: 338.2558)\n",
            "Epoch 10/50 - Beta: 1.0000 - Train Loss: 2567.8244 (Rec: 2252.2440, KL: 315.5803) - Val Loss: 2630.2415 (Rec: 2296.6634, KL: 333.5781)\n",
            "Epoch 11/50 - Beta: 1.0000 - Train Loss: 2487.9058 (Rec: 2177.2649, KL: 310.6409) - Val Loss: 2521.4307 (Rec: 2196.2905, KL: 325.1402)\n",
            "Epoch 12 - Early stopping counter: 1/5\n",
            "Epoch 12/50 - Beta: 1.0000 - Train Loss: 2404.2648 (Rec: 2096.2482, KL: 308.0166) - Val Loss: 2535.8439 (Rec: 2227.9766, KL: 307.8673)\n",
            "Epoch 13/50 - Beta: 1.0000 - Train Loss: 2333.7821 (Rec: 2027.5579, KL: 306.2241) - Val Loss: 2454.9983 (Rec: 2133.4419, KL: 321.5563)\n",
            "Epoch 14/50 - Beta: 1.0000 - Train Loss: 2272.7568 (Rec: 1968.5844, KL: 304.1725) - Val Loss: 2334.0800 (Rec: 2030.1201, KL: 303.9599)\n",
            "Epoch 15/50 - Beta: 1.0000 - Train Loss: 2217.4221 (Rec: 1917.4521, KL: 299.9701) - Val Loss: 2301.3425 (Rec: 1997.7301, KL: 303.6124)\n",
            "Epoch 16 - Early stopping counter: 1/5\n",
            "Epoch 16/50 - Beta: 1.0000 - Train Loss: 2169.1362 (Rec: 1869.8244, KL: 299.3118) - Val Loss: 2313.3692 (Rec: 2011.3743, KL: 301.9948)\n",
            "Epoch 17/50 - Beta: 1.0000 - Train Loss: 2128.0620 (Rec: 1830.1965, KL: 297.8655) - Val Loss: 2212.4306 (Rec: 1917.5257, KL: 294.9049)\n",
            "Epoch 18/50 - Beta: 1.0000 - Train Loss: 2075.1965 (Rec: 1780.6581, KL: 294.5384) - Val Loss: 2205.1356 (Rec: 1888.0488, KL: 317.0868)\n",
            "Epoch 19/50 - Beta: 1.0000 - Train Loss: 2038.7705 (Rec: 1745.8050, KL: 292.9655) - Val Loss: 2156.3601 (Rec: 1842.4041, KL: 313.9560)\n",
            "Epoch 20/50 - Beta: 1.0000 - Train Loss: 1998.4152 (Rec: 1706.5825, KL: 291.8327) - Val Loss: 2133.4764 (Rec: 1821.8757, KL: 311.6006)\n",
            "Epoch 21 - Early stopping counter: 1/5\n",
            "Epoch 21/50 - Beta: 1.0000 - Train Loss: 1965.5400 (Rec: 1675.7658, KL: 289.7742) - Val Loss: 2135.7328 (Rec: 1807.4014, KL: 328.3314)\n",
            "Epoch 22/50 - Beta: 1.0000 - Train Loss: 1936.3479 (Rec: 1646.2881, KL: 290.0597) - Val Loss: 2073.7282 (Rec: 1793.1606, KL: 280.5677)\n",
            "Epoch 23/50 - Beta: 1.0000 - Train Loss: 1912.2553 (Rec: 1623.1370, KL: 289.1183) - Val Loss: 2070.9727 (Rec: 1793.2605, KL: 277.7121)\n",
            "Epoch 24/50 - Beta: 1.0000 - Train Loss: 1880.1197 (Rec: 1593.1275, KL: 286.9922) - Val Loss: 2039.0276 (Rec: 1721.2706, KL: 317.7571)\n",
            "Epoch 25/50 - Beta: 1.0000 - Train Loss: 1854.3936 (Rec: 1568.1567, KL: 286.2369) - Val Loss: 2009.4734 (Rec: 1725.8845, KL: 283.5889)\n",
            "Epoch 26/50 - Beta: 1.0000 - Train Loss: 1829.3235 (Rec: 1543.0263, KL: 286.2972) - Val Loss: 2008.4922 (Rec: 1703.2173, KL: 305.2748)\n",
            "Epoch 27/50 - Beta: 1.0000 - Train Loss: 1806.0724 (Rec: 1520.6150, KL: 285.4574) - Val Loss: 1976.7930 (Rec: 1696.9752, KL: 279.8178)\n",
            "Epoch 28/50 - Beta: 1.0000 - Train Loss: 1788.1875 (Rec: 1502.7867, KL: 285.4008) - Val Loss: 1962.9117 (Rec: 1657.7571, KL: 305.1546)\n",
            "Epoch 29/50 - Beta: 1.0000 - Train Loss: 1765.2677 (Rec: 1479.6851, KL: 285.5827) - Val Loss: 1940.1574 (Rec: 1670.8416, KL: 269.3157)\n",
            "Epoch 30/50 - Beta: 1.0000 - Train Loss: 1749.6826 (Rec: 1464.7806, KL: 284.9020) - Val Loss: 1924.4590 (Rec: 1650.6333, KL: 273.8258)\n",
            "Epoch 31/50 - Beta: 1.0000 - Train Loss: 1730.0625 (Rec: 1446.2043, KL: 283.8583) - Val Loss: 1918.3052 (Rec: 1637.9776, KL: 280.3276)\n",
            "Epoch 32/50 - Beta: 1.0000 - Train Loss: 1714.1519 (Rec: 1431.7121, KL: 282.4397) - Val Loss: 1906.5000 (Rec: 1614.5688, KL: 291.9312)\n",
            "Epoch 33/50 - Beta: 1.0000 - Train Loss: 1700.2552 (Rec: 1417.1003, KL: 283.1549) - Val Loss: 1895.0606 (Rec: 1610.7305, KL: 284.3300)\n",
            "Epoch 34 - Early stopping counter: 1/5\n",
            "Epoch 34/50 - Beta: 1.0000 - Train Loss: 1689.6424 (Rec: 1405.7356, KL: 283.9068) - Val Loss: 1896.2361 (Rec: 1586.6969, KL: 309.5393)\n",
            "Epoch 35/50 - Beta: 1.0000 - Train Loss: 1675.5084 (Rec: 1392.5397, KL: 282.9687) - Val Loss: 1877.5983 (Rec: 1602.7684, KL: 274.8299)\n",
            "Epoch 36/50 - Beta: 1.0000 - Train Loss: 1662.3983 (Rec: 1378.8763, KL: 283.5220) - Val Loss: 1870.4716 (Rec: 1585.5402, KL: 284.9314)\n",
            "Epoch 37/50 - Beta: 1.0000 - Train Loss: 1653.3245 (Rec: 1370.2392, KL: 283.0853) - Val Loss: 1859.5088 (Rec: 1581.3925, KL: 278.1163)\n",
            "Epoch 38/50 - Beta: 1.0000 - Train Loss: 1644.6313 (Rec: 1361.1010, KL: 283.5303) - Val Loss: 1854.5730 (Rec: 1565.0161, KL: 289.5568)\n",
            "Epoch 39/50 - Beta: 1.0000 - Train Loss: 1635.3770 (Rec: 1352.2537, KL: 283.1233) - Val Loss: 1848.5557 (Rec: 1566.2308, KL: 282.3249)\n",
            "Epoch 40/50 - Beta: 1.0000 - Train Loss: 1627.3652 (Rec: 1344.5641, KL: 282.8011) - Val Loss: 1844.7244 (Rec: 1566.5983, KL: 278.1261)\n",
            "Epoch 41/50 - Beta: 1.0000 - Train Loss: 1621.6905 (Rec: 1338.8664, KL: 282.8241) - Val Loss: 1837.9312 (Rec: 1561.5020, KL: 276.4291)\n",
            "Epoch 42/50 - Beta: 1.0000 - Train Loss: 1613.7943 (Rec: 1331.7719, KL: 282.0224) - Val Loss: 1833.7874 (Rec: 1550.5782, KL: 283.2092)\n",
            "Epoch 43/50 - Beta: 1.0000 - Train Loss: 1607.9106 (Rec: 1326.4895, KL: 281.4210) - Val Loss: 1829.3795 (Rec: 1548.6992, KL: 280.6803)\n",
            "Epoch 44/50 - Beta: 1.0000 - Train Loss: 1605.0529 (Rec: 1323.0927, KL: 281.9602) - Val Loss: 1828.4590 (Rec: 1547.3268, KL: 281.1322)\n",
            "Epoch 45 - Early stopping counter: 1/5\n",
            "Epoch 45/50 - Beta: 1.0000 - Train Loss: 1601.2725 (Rec: 1319.1295, KL: 282.1430) - Val Loss: 1830.6056 (Rec: 1544.0536, KL: 286.5520)\n",
            "Epoch 46/50 - Beta: 1.0000 - Train Loss: 1598.1245 (Rec: 1315.9433, KL: 282.1812) - Val Loss: 1824.0192 (Rec: 1539.7256, KL: 284.2936)\n",
            "Epoch 47 - Early stopping counter: 1/5\n",
            "Epoch 47/50 - Beta: 1.0000 - Train Loss: 1597.1293 (Rec: 1314.4786, KL: 282.6507) - Val Loss: 1827.4732 (Rec: 1538.1444, KL: 289.3287)\n",
            "Epoch 48 - Early stopping counter: 2/5\n",
            "Epoch 48/50 - Beta: 1.0000 - Train Loss: 1594.4055 (Rec: 1313.0961, KL: 281.3093) - Val Loss: 1826.7672 (Rec: 1543.9095, KL: 282.8577)\n",
            "Epoch 49/50 - Beta: 1.0000 - Train Loss: 1593.7883 (Rec: 1312.1164, KL: 281.6720) - Val Loss: 1822.6383 (Rec: 1541.3382, KL: 281.3001)\n",
            "Epoch 50 - Early stopping counter: 1/5\n",
            "Epoch 50/50 - Beta: 1.0000 - Train Loss: 1593.5514 (Rec: 1311.8411, KL: 281.7103) - Val Loss: 1824.4657 (Rec: 1542.6449, KL: 281.8208)\n",
            "Test Results - Loss: 1820.2270 (Rec: 1534.9419, KL: 285.2851)\n",
            "Training complete. Final model saved as 'video_vae_final.pth'.\n",
            "Copied /content/drive/MyDrive/Herts - BSc /3rd Year/FYP/Workflow/Final-WorkFlow/Architecture-Training.ipynb to /content/drive/MyDrive/Herts - BSc /3rd Year/FYP/trained_models/vae_model_v10.02/Architecture-Training.ipynb\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}